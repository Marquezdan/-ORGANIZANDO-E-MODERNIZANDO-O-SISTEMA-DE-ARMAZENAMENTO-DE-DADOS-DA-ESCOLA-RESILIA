{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marquezdan/-ORGANIZANDO-E-MODERNIZANDO-O-SISTEMA-DE-ARMAZENAMENTO-DE-DADOS-DA-ESCOLA-RESILIA/blob/main/Trabalho_de_Machine_Learning_Estat%C3%ADstico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Exercício de Machine Learning - Redes Neurais para Aproximação de Funções\n",
        "\n",
        "## Introdução\n",
        "\n",
        "Neste exercício, vamos explorar o funcionamento de uma rede neural artificial simples. A rede foi projetada para aproximar a função seno, ou seja, aprender a representar a relação entre entradas e saídas que seguem o padrão da função seno. Através desse exercício, você será capaz de entender os conceitos básicos do treinamento de redes neurais e como diferentes parâmetros afetam seu desempenho.\n",
        "\n",
        "# Objetivo\n",
        "\n",
        "A rede neural que vamos utilizar aqui é um modelo de aprendizado supervisionado, que aprende a partir de exemplos de entrada e saída (pares de dados). Nosso objetivo é que a rede seja capaz de prever o valor de uma função seno dada uma entrada específica, mesmo quando os dados de entrada estejam contaminados com algum ruído (para simular situações reais, onde os dados nem sempre são perfeitos).\n",
        "\n",
        "# Estrutura do Exercício\n",
        "\n",
        "Ao longo do notebook, você encontrará tarefas práticas que vão guiá-lo através dos principais conceitos de treinamento de redes neurais. Essas tarefas vão permitir que você:\n",
        "\n",
        "\t1.\tModifique funções de ativação para entender como elas impactam o aprendizado.\n",
        "\t2.\tAjuste o número de épocas para observar a convergência da rede.\n",
        "\t3.\tExperimente diferentes taxas de aprendizado para ver como elas afetam a estabilidade e a rapidez do treinamento.\n",
        "\t4.\tAltere a quantidade de neurônios e camadas para explorar a capacidade da rede de representar padrões complexos.\n",
        "\t5.\tAplique regularização para melhorar a capacidade da rede de generalizar para novos dados.\n",
        "\n",
        "Cada tarefa inclui uma descrição do que precisa ser feito e instruções sobre como modificar o código e visualizar os resultados. O propósito é que você possa experimentar com a rede neural, observar os resultados das suas alterações e desenvolver uma compreensão intuitiva de como esses parâmetros afetam o comportamento do modelo.\n",
        "\n",
        "# O que Você Vai Aprender\n",
        "\n",
        "Ao completar este exercício, você terá um entendimento básico de como redes neurais processam informações, ajustam seus parâmetros e fazem previsões. Além disso, você entenderá melhor como configurá-las para resolver problemas práticos, como aproximação de funções e predição em cenários com ruído.\n",
        "\n",
        "Vamos começar e observar como essa rede neural simples pode aprender a forma de uma função seno!"
      ],
      "metadata": {
        "id": "6mdUOTPlT9gn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zA2U6xiT8Y-"
      },
      "outputs": [],
      "source": [
        "#Daniel marques\n",
        "#Importação de Bibliotecas e Configurações Iniciais\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parâmetros principais\n",
        "SEED = 42\n",
        "N_SAMPLES = 200\n",
        "LAYERS = [1, 10, 10, 10, 1]\n",
        "LEARNING_RATE = 0.05\n",
        "N_EPOCHS = 10000\n",
        "\n",
        "# Configuração da semente aleatória\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Geração dos dados de entrada e saída com ruído\n",
        "x_samples = np.random.uniform(0, 2 * np.pi, (1, N_SAMPLES))\n",
        "y_samples = np.sin(x_samples) + np.random.normal(0.0, 0.3, (1, N_SAMPLES))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, definimos várias funções de ativação que a rede neural pode usar: sigmoid, gelu, identity (para a camada de saída), e relu. Em seguida, criamos a função initialize_network, que configura a rede neural, inicializando os pesos, os vieses e associando a função de ativação a cada camada."
      ],
      "metadata": {
        "id": "lC_xEMFlXEAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição de funções de ativação\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Função Sigmoide.\"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivada da função Sigmoide em relação a z.\"\"\"\n",
        "    a = sigmoid(z)\n",
        "    return a * (1 - a)\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Função de ativação GELU.\"\"\"\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
        "\n",
        "def gelu_prime(z):\n",
        "    \"\"\"Derivada da função GELU em relação a z.\"\"\"\n",
        "    sqrt_2_over_pi = np.sqrt(2 / np.pi)\n",
        "    term = sqrt_2_over_pi * (z + 0.044715 * np.power(z, 3))\n",
        "    tanh_out = np.tanh(term)\n",
        "    return 0.5 * (1 + tanh_out + z * (1 - tanh_out**2) * (sqrt_2_over_pi + 0.134145 * z**2))\n",
        "\n",
        "def identity(x):\n",
        "    \"\"\"Função identidade, utilizada na camada de saída.\"\"\"\n",
        "    return x\n",
        "\n",
        "def identity_prime(z):\n",
        "    \"\"\"Derivada da função identidade em relação a z.\"\"\"\n",
        "    return np.ones_like(z)\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"Função de ativação ReLU.\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_prime(z):\n",
        "    \"\"\"Derivada da função ReLU em relação a z.\"\"\"\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "# Inicializando a rede neural com funções de ativação como parâmetros\n",
        "\n",
        "def initialize_network(layers, activation_function, activation_derivative):\n",
        "    \"\"\"\n",
        "    Inicializa a rede neural com a arquitetura especificada, função de ativação e sua derivada.\n",
        "\n",
        "    Args:\n",
        "        layers (list): Lista contendo o número de neurônios em cada camada.\n",
        "        activation_function (function): Função de ativação a ser usada nas camadas ocultas.\n",
        "        activation_derivative (function): Derivada da função de ativação.\n",
        "\n",
        "    Returns:\n",
        "        weight_matrices (list): Lista de matrizes de pesos.\n",
        "        bias_vectors (list): Lista de vetores de bias.\n",
        "        activation_functions (list): Lista de funções de ativação para cada camada.\n",
        "        activations_derivatives (list): Lista de derivadas das funções de ativação para cada camada.\n",
        "    \"\"\"\n",
        "    weight_matrices = []\n",
        "    bias_vectors = []\n",
        "    activation_functions = [activation_function] * (len(layers) - 2) + [identity]  # Define as ativações\n",
        "    activations_derivatives = [activation_derivative] * (len(layers) - 2) + [identity_prime]  # Define as derivadas\n",
        "\n",
        "    for fan_in, fan_out in zip(layers[:-1], layers[1:]):\n",
        "        kernel_matrix_uniform_limit = np.sqrt(6 / (fan_in + fan_out))\n",
        "        W = np.random.uniform(-kernel_matrix_uniform_limit, kernel_matrix_uniform_limit, (fan_out, fan_in))\n",
        "        b = np.zeros((fan_out, 1))\n",
        "        weight_matrices.append(W)\n",
        "        bias_vectors.append(b)\n",
        "\n",
        "    return weight_matrices, bias_vectors, activation_functions, activations_derivatives"
      ],
      "metadata": {
        "id": "bRZs6ydlUvCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções de Passagem para Frente (Forward) e Cálculo da Perda\n",
        "\n",
        "Implementamos duas funções importantes:\n",
        "\n",
        "\t1) network_forward: executa a passagem para frente (forward pass) da rede, calculando a saída final da rede.\n",
        "\t2 )loss_forward: calcula a perda (erro) entre a saída da rede e o valor real, usando o erro quadrático médio e um termo de regularização L2 opcional."
      ],
      "metadata": {
        "id": "IaI1WiYdjbGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def network_forward(x, weights, biases, activations):\n",
        "    \"\"\"\n",
        "    Executa a passagem para frente da rede neural.\n",
        "\n",
        "    Args:\n",
        "        x (numpy.ndarray): Entrada para a rede.\n",
        "        weights (list): Lista de matrizes de pesos.\n",
        "        biases (list): Lista de vetores de bias.\n",
        "        activations (list): Lista de funções de ativação para cada camada.\n",
        "\n",
        "    Returns:\n",
        "        a (numpy.ndarray): Saída final da rede.\n",
        "    \"\"\"\n",
        "    a = x\n",
        "    for W, b, activation in zip(weights, biases, activations):\n",
        "        z = np.dot(W, a) + b\n",
        "        a = activation(z)\n",
        "    return a\n",
        "\n",
        "def loss_forward(y, y_ref, weights=None, lambda_l2=0):\n",
        "    \"\"\"\n",
        "    Calcula a perda entre a saída da rede e o valor real.\n",
        "\n",
        "    Args:\n",
        "        y (numpy.ndarray): Saída da rede.\n",
        "        y_ref (numpy.ndarray): Valor real.\n",
        "        weights (list, optional): Lista de matrizes de pesos para regularização L2.\n",
        "        lambda_l2 (float, optional): Fator de regularização L2.\n",
        "\n",
        "    Returns:\n",
        "        loss (float): Valor da perda.\n",
        "    \"\"\"\n",
        "    delta = y - y_ref\n",
        "    loss = 0.5 * np.mean(np.sum(delta ** 2, axis=0))\n",
        "    if lambda_l2 > 0 and weights is not None:\n",
        "        l2_penalty = lambda_l2 * sum(np.sum(W ** 2) for W in weights)\n",
        "        loss += l2_penalty\n",
        "    return loss"
      ],
      "metadata": {
        "id": "caYcKL_pU7Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Função de Backpropagation**\n",
        "\n",
        "A função network_forward_and_backward calcula o processo de backpropagation, que permite ajustar os pesos e vieses da rede. Ela realiza a passagem para trás (backward pass), calculando os gradientes da função de perda em relação aos parâmetros da rede, para que possam ser atualizados durante o treinamento."
      ],
      "metadata": {
        "id": "MBKfU3BuXJt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def network_forward_and_backward(x, y_ref, weights, biases, activations, activations_derivatives, lambda_l2=0):\n",
        "    \"\"\"\n",
        "    Executa a passagem para frente e o backpropagation, calculando os gradientes.\n",
        "\n",
        "    Args:\n",
        "        x (numpy.ndarray): Entrada para a rede.\n",
        "        y_ref (numpy.ndarray): Valor real.\n",
        "        weights (list): Lista de matrizes de pesos.\n",
        "        biases (list): Lista de vetores de bias.\n",
        "        activations (list): Lista de funções de ativação para cada camada.\n",
        "        activations_derivatives (list): Lista de derivadas das funções de ativação para cada camada.\n",
        "        lambda_l2 (float, optional): Fator de regularização L2.\n",
        "\n",
        "    Returns:\n",
        "        loss (float): Valor da perda.\n",
        "        weight_gradients (list): Lista de gradientes dos pesos.\n",
        "        bias_gradients (list): Lista de gradientes dos bias.\n",
        "    \"\"\"\n",
        "    # Passagem para frente\n",
        "    a = x\n",
        "    layer_values = [a]\n",
        "    z_values = []  # Lista para armazenar os valores de z\n",
        "\n",
        "    for W, b, activation in zip(weights, biases, activations):\n",
        "        z = np.dot(W, a) + b\n",
        "        a = activation(z)\n",
        "        z_values.append(z)\n",
        "        layer_values.append(a)\n",
        "\n",
        "    y = a\n",
        "    loss = loss_forward(y, y_ref, weights, lambda_l2)\n",
        "\n",
        "    # Passagem para trás (backpropagation)\n",
        "    current_cotangent = (y - y_ref) / y.shape[1]  # Gradiente da camada de saída\n",
        "    weights_gradients = []\n",
        "    bias_gradients = []\n",
        "\n",
        "    # Retropropagação dos gradientes\n",
        "    for W, activation_prime, z_current, a_prev in zip(\n",
        "            reversed(weights), reversed(activations_derivatives),\n",
        "            reversed(z_values), reversed(layer_values[:-1])):\n",
        "\n",
        "        # Calcula o gradiente com base no pré-ativação z_current\n",
        "        plus_bias_state_cotangent = current_cotangent * activation_prime(z_current)\n",
        "\n",
        "        # Gradiente do bias\n",
        "        bias_grad = np.sum(plus_bias_state_cotangent, axis=1, keepdims=True)\n",
        "        bias_gradients.append(bias_grad)\n",
        "\n",
        "        # Gradiente dos pesos com regularização L2 adicionada\n",
        "        weight_grad = np.dot(plus_bias_state_cotangent, a_prev.T) + lambda_l2 * W\n",
        "        weights_gradients.append(weight_grad)\n",
        "\n",
        "        # Atualiza o cotangent para a próxima camada\n",
        "        current_cotangent = np.dot(W.T, plus_bias_state_cotangent)\n",
        "\n",
        "    # Inverte as listas de gradientes para manter a ordem original das camadas\n",
        "    return loss, list(reversed(weights_gradients)), list(reversed(bias_gradients))"
      ],
      "metadata": {
        "id": "4gfMLNisWdiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A função train_network utiliza as funções anteriores para treinar a rede neural ao longo de múltiplas épocas, atualizando os pesos e vieses com base nos gradientes calculados na função de backpropagation."
      ],
      "metadata": {
        "id": "pP8uBdptXt8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pzffET2BXOpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_network(n_epochs, learning_rate, activation_functions, activations_derivatives, weights, biases, lambda_l2=0):\n",
        "    \"\"\"\n",
        "    Treina a rede neural por um número especificado de épocas.\n",
        "\n",
        "    Args:\n",
        "        n_epochs (int): Número de épocas para o treinamento.\n",
        "        learning_rate (float): Taxa de aprendizado.\n",
        "        activation_functions (list): Lista de funções de ativação para cada camada.\n",
        "        activations_derivatives (list): Lista de derivadas das funções de ativação para cada camada.\n",
        "        weights (list): Lista de matrizes de pesos.\n",
        "        biases (list): Lista de vetores de bias.\n",
        "        lambda_l2 (float, optional): Fator de regularização L2.\n",
        "\n",
        "    Returns:\n",
        "        loss_history (list): Histórico das perdas ao longo das épocas.\n",
        "        weights (list): Pesos atualizados após o treinamento.\n",
        "        biases (list): Biases atualizados após o treinamento.\n",
        "    \"\"\"\n",
        "    loss_history = []\n",
        "    for epoch in range(n_epochs):\n",
        "        loss, weight_grads, bias_grads = network_forward_and_backward(\n",
        "            x_samples,\n",
        "            y_samples,\n",
        "            weights,\n",
        "            biases,\n",
        "            activation_functions,\n",
        "            activations_derivatives,\n",
        "            lambda_l2\n",
        "        )\n",
        "\n",
        "        # Atualização dos pesos e biases\n",
        "        for W, W_grad, b, b_grad in zip(weights, weight_grads, biases, bias_grads):\n",
        "            W -= learning_rate * W_grad\n",
        "            b -= learning_rate * b_grad\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch: {epoch}, loss: {loss:.5f}\")\n",
        "\n",
        "        loss_history.append(loss)\n",
        "\n",
        "    return loss_history, weights, biases"
      ],
      "metadata": {
        "id": "ipBPogNaWk76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, vamos definir duas funções de visualização:\n",
        "\n",
        "1) plot_loss: plota a perda ao longo das épocas, para que possamos visualizar a convergência do modelo.\n",
        "\n",
        "2) plot_predictions: compara as previsões da rede com os dados reais, para vermos como a rede conseguiu aproximar a função seno."
      ],
      "metadata": {
        "id": "iFuMcIzKYXPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para visualização da curva de perda\n",
        "def plot_loss(loss_history):\n",
        "    \"\"\"\n",
        "    Plota a curva de perda ao longo das épocas.\n",
        "\n",
        "    Args:\n",
        "        loss_history (list): Histórico das perdas.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(loss_history)\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"Perda\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.title(\"Perda ao longo das épocas\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Função para visualização dos resultados de predição\n",
        "def plot_predictions(x_samples, y_samples, weights, biases, activations):\n",
        "    \"\"\"\n",
        "    Plota as predições da rede neural em comparação com os dados reais.\n",
        "\n",
        "    Args:\n",
        "        x_samples (numpy.ndarray): Dados de entrada.\n",
        "        y_samples (numpy.ndarray): Dados reais.\n",
        "        weights (list): Lista de matrizes de pesos.\n",
        "        biases (list): Lista de vetores de bias.\n",
        "        activations (list): Lista de funções de ativação para cada camada.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.scatter(x_samples.flatten(), y_samples.flatten(), label=\"Dados reais\", alpha=0.6)\n",
        "    y_pred = network_forward(x_samples, weights, biases, activations)\n",
        "    plt.scatter(x_samples.flatten(), y_pred.flatten(), label=\"Predição da rede\", color=\"red\", alpha=0.6)\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Predições da rede neural em comparação com os dados reais\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YIlC0mUmWvcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A função train_network utiliza as funções anteriores para treinar a rede neural ao longo de múltiplas épocas, atualizando os pesos e vieses com base nos gradientes calculados na função de backpropagation."
      ],
      "metadata": {
        "id": "vFAMvwreYQUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializando a rede neural com a função de ativação GELU\n",
        "weights, biases, activation_functions, activations_derivatives = initialize_network(LAYERS, gelu, gelu_prime)\n",
        "\n",
        "# Treinando a rede neural\n",
        "loss_history, updated_weights, updated_biases = train_network(\n",
        "    n_epochs=N_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    activation_functions=activation_functions,\n",
        "    activations_derivatives=activations_derivatives,\n",
        "    weights=weights,\n",
        "    biases=biases,\n",
        "    lambda_l2=0  # Você pode ajustar este valor conforme necessário\n",
        ")\n",
        "\n",
        "# Plotando a curva de perda\n",
        "plot_loss(loss_history)\n",
        "\n",
        "# Plotando as predições da rede em comparação com os dados reais\n",
        "plot_predictions(x_samples, y_samples, updated_weights, updated_biases, activation_functions)"
      ],
      "metadata": {
        "id": "HF8HyLzYWzXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KempPE0QYJl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliação da Rede Neural com Dados de Teste\n",
        "\n",
        "Objetivo: Após treinar a rede neural com os dados de treinamento, é fundamental avaliar seu desempenho em um conjunto de dados de teste independente. Essa avaliação nos permite verificar a capacidade de generalização do modelo, ou seja, sua habilidade de realizar predições precisas em dados que não foram utilizados durante o treinamento.\n"
      ],
      "metadata": {
        "id": "LCkUrIfKlpXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerando dados de teste\n",
        "x_test = np.random.uniform(0, 2 * np.pi, (1, 200))\n",
        "y_test = np.sin(x_test) + np.random.normal(0.0, 0.3, (1, 200))\n",
        "\n",
        "# Fazendo predições no conjunto de teste usando a rede treinada\n",
        "y_pred_test = network_forward(x_test, weights, biases, activation_functions).flatten()\n",
        "\n",
        "# Calculando a perda no conjunto de teste\n",
        "test_loss = loss_forward(y_pred_test, y_test)\n",
        "print(f\"Test Loss: {test_loss:.5f}\")\n",
        "\n",
        "# Plotando as predições no conjunto de teste\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(x_test.flatten(), y_test.flatten(), label=\"Dados de Teste\", alpha=0.6)\n",
        "plt.scatter(x_test.flatten(), y_pred_test, label=\"Predição da Rede\", color=\"red\", alpha=0.6)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.title(\"Predições da Rede Neural no Conjunto de Teste\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_coZtVoClpuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarefas de Experimentação\n",
        "\n",
        "Abaixo, estão as tarefas que você deve realizar para explorar o comportamento da rede neural com diferentes parâmetros. Para cada tarefa, leia as instruções, faça as alterações sugeridas e observe como o desempenho da rede é afetado.\n",
        "\n",
        "# Tarefa 1: Exploração de Funções de Ativação\n",
        "\n",
        "**Instruções**\n",
        "\n",
        "Nesta tarefa, você explorará o impacto de diferentes funções de ativação no desempenho da rede neural. A função de ativação controla como os neurônios transformam os dados de entrada, e diferentes funções podem influenciar a capacidade de aprendizado da rede.\n",
        "\n",
        "\t1.\tObjetivo: Observar o impacto de diferentes funções de ativação.\n",
        "\t2.\tAltere a função initialize_network para usar sigmoid ou relu como função de ativação no lugar de gelu.\n",
        "\t3.\tExecute o treinamento e observe o comportamento da curva de perda para cada função.\n",
        "\n",
        "  Obs:. Note que o LEARNING RATE foi ajustado para a função GELU\n"
      ],
      "metadata": {
        "id": "J8wcHnmcYeGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de visualização de separação\n",
        "def initialize_and_train(layers, activation_func, activation_derivative, lr, epochs, x_train, y_train, label):\n",
        "    \"\"\"\n",
        "    Inicializa a rede com uma função de ativação específica, treina e plota os resultados.\n",
        "\n",
        "    Args:\n",
        "        layers (list): Arquitetura da rede.\n",
        "        activation_func (function): Função de ativação.\n",
        "        activation_derivative (function): Derivada da função de ativação.\n",
        "        lr (float): Taxa de aprendizado.\n",
        "        epochs (int): Número de épocas de treinamento.\n",
        "        x_train (numpy.ndarray): Dados de entrada para treinamento.\n",
        "        y_train (numpy.ndarray): Dados de saída para treinamento.\n",
        "        label (str): Rótulo para identificação da função de ativação.\n",
        "\n",
        "    Returns:\n",
        "        updated_weights (list): Pesos atualizados após o treinamento.\n",
        "        updated_biases (list): Biases atualizados após o treinamento.\n",
        "    \"\"\"\n",
        "    weights, biases, activation_functions, activations_derivatives = initialize_network(layers, activation_func, activation_derivative)\n",
        "    print(f\"\\nTreinando com ativação {label}...\")\n",
        "    loss_history, updated_weights, updated_biases = train_network(\n",
        "        epochs,\n",
        "        lr,\n",
        "        activation_functions,\n",
        "        activations_derivatives,\n",
        "        weights,\n",
        "        biases\n",
        "    )\n",
        "    plot_loss(loss_history)\n",
        "    plot_predictions(x_train, y_train, updated_weights, updated_biases, activation_functions)\n",
        "    return updated_weights, updated_biases  # Retorna pesos e biases atualizados, se necessário\n",
        "LEARNING_RATE=0.1 => ajuste o lr para a Sigmoid e ReLU\n",
        "# Testando com ativação Sigmoid\n",
        "updated_weights_sigmoid, updated_biases_sigmoid = initialize_and_train(\n",
        "    layers=LAYERS,\n",
        "    activation_func=sigmoid,\n",
        "    activation_derivative=sigmoid_prime,\n",
        "    lr=LEARNING_RATE,\n",
        "    epochs=N_EPOCHS,\n",
        "    x_train=x_samples,\n",
        "    y_train=y_samples,\n",
        "    label=\"Sigmoid\"\n",
        ")\n",
        "\n",
        "# Testando com ativação ReLU\n",
        "updated_weights_relu, updated_biases_relu = initialize_and_train(\n",
        "    layers=LAYERS,\n",
        "    activation_func=relu,\n",
        "    activation_derivative=relu_prime,\n",
        "    lr=LEARNING_RATE,\n",
        "    epochs=N_EPOCHS,\n",
        "    x_train=x_samples,\n",
        "    y_train=y_samples,\n",
        "    label=\"ReLU\"\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "WqQoBKs4Ypr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarefa 2: Variação do Número de Épocas\n",
        "\n",
        "Objetivo: Analisar como a quantidade de épocas de treinamento influencia o desempenho da rede neural na aproximação da função seno. Observaremos como diferentes números de épocas afetam a convergência da perda e a qualidade das predições da rede.\n",
        "\n"
      ],
      "metadata": {
        "id": "oMZ_vriy5Mbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tarefa 2: Variação do Número de Épocas\n",
        "\n",
        "# Definindo diferentes números de épocas para treinamento\n",
        "epochs_list = [1000, 5000, 10000, 20000]\n",
        "\n",
        "# Armazenando os históricos de perda para cada configuração\n",
        "loss_histories = {}\n",
        "\n",
        "for epochs in epochs_list:\n",
        "    print(f\"\\nTreinando com {epochs} épocas...\")\n",
        "    # Inicializando a rede neural com a função de ativação GELU\n",
        "    weights, biases, activation_functions, activations_derivatives = initialize_network(LAYERS, gelu, gelu_prime)\n",
        "\n",
        "    # Treinando a rede neural\n",
        "    loss_history, updated_weights, updated_biases = train_network(\n",
        "        n_epochs=epochs,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        activation_functions=activation_functions,\n",
        "        activations_derivatives=activations_derivatives,\n",
        "        weights=weights,\n",
        "        biases=biases,\n",
        "        lambda_l2=0  # Sem regularização L2 nesta tarefa\n",
        "    )\n",
        "\n",
        "    # Armazenando o histórico de perda\n",
        "    loss_histories[epochs] = loss_history\n",
        "\n",
        "    # Plotando as predições da rede em comparação com os dados reais\n",
        "    plot_predictions(x_samples, y_samples, updated_weights, updated_biases, activation_functions)\n",
        "\n",
        "# Plotando todas as curvas de perda em um único gráfico para comparação\n",
        "plt.figure(figsize=(10, 6))\n",
        "for epochs, loss_history in loss_histories.items():\n",
        "    plt.plot(loss_history, label=f\"{epochs} Épocas\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Perda\")\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"Comparação das Curvas de Perda com Diferente Número de Épocas\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vZ6Za9CN5NGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Tarefa 3: Influência da Taxa de Aprendizado\n",
        "\n",
        "Objetivo:Investigar como diferentes taxas de aprendizado (learning_rate) afetam a convergência e o desempenho da rede neural. Analisaremos como a velocidade e a estabilidade do treinamento variam com a alteração dessa taxa."
      ],
      "metadata": {
        "id": "CEunEY7q5n73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tarefa 3: Influência da Taxa de Aprendizado\n",
        "\n",
        "# Definindo diferentes taxas de aprendizado para treinamento\n",
        "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
        "\n",
        "# Armazenando os históricos de perda para cada configuração\n",
        "loss_histories_lr = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\nTreinando com Taxa de Aprendizado: {lr}\")\n",
        "    # Inicializando a rede neural com a função de ativação GELU\n",
        "    weights, biases, activation_functions, activations_derivatives = initialize_network(LAYERS, gelu, gelu_prime)\n",
        "\n",
        "    # Treinando a rede neural\n",
        "    loss_history, updated_weights, updated_biases = train_network(\n",
        "        n_epochs=N_EPOCHS,\n",
        "        learning_rate=lr,\n",
        "        activation_functions=activation_functions,\n",
        "        activations_derivatives=activations_derivatives,\n",
        "        weights=weights,\n",
        "        biases=biases,\n",
        "        lambda_l2=0  # Sem regularização L2 nesta tarefa\n",
        "    )\n",
        "\n",
        "    # Armazenando o histórico de perda\n",
        "    loss_histories_lr[lr] = loss_history\n",
        "\n",
        "    # Plotando as predições da rede em comparação com os dados reais\n",
        "    plot_predictions(x_samples, y_samples, updated_weights, updated_biases, activation_functions)\n",
        "\n",
        "# Plotando todas as curvas de perda em um único gráfico para comparação\n",
        "plt.figure(figsize=(10, 6))\n",
        "for lr, loss_history in loss_histories_lr.items():\n",
        "    plt.plot(loss_history, label=f\"LR={lr}\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Perda\")\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"Comparação das Curvas de Perda com Diferentes Taxas de Aprendizado\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IQ5sVcEI5oOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarefa 4: Ajuste do Número de Neurônios por Camada\n",
        "\n",
        "Objetivo: Avaliar como a quantidade de neurônios em cada camada oculta influencia a capacidade de aprendizagem e a performance da rede neural. Exploraremos diferentes arquiteturas para entender o impacto da complexidade da rede."
      ],
      "metadata": {
        "id": "sqp7HcoI58EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tarefa 4: Ajuste do Número de Neurônios por Camada\n",
        "\n",
        "# Definindo diferentes configurações de número de neurônios por camada\n",
        "neurons_configurations = [\n",
        "    [1, 5, 5, 5, 1],\n",
        "    [1, 10, 10, 10, 1],\n",
        "    [1, 20, 20, 20, 1],\n",
        "    [1, 50, 50, 50, 1]\n",
        "]\n",
        "\n",
        "# Armazenando os históricos de perda para cada configuração\n",
        "loss_histories_neurons = {}\n",
        "\n",
        "for neurons in neurons_configurations:\n",
        "    print(f\"\\nTreinando com configuração de neurônios: {neurons}\")\n",
        "    # Inicializando a rede neural com a função de ativação GELU\n",
        "    weights, biases, activation_functions, activations_derivatives = initialize_network(neurons, gelu, gelu_prime)\n",
        "\n",
        "    # Treinando a rede neural\n",
        "    loss_history, updated_weights, updated_biases = train_network(\n",
        "        n_epochs=N_EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        activation_functions=activation_functions,\n",
        "        activations_derivatives=activations_derivatives,\n",
        "        weights=weights,\n",
        "        biases=biases,\n",
        "        lambda_l2=0  # Sem regularização L2 nesta tarefa\n",
        "    )\n",
        "\n",
        "    # Armazenando o histórico de perda\n",
        "    loss_histories_neurons[tuple(neurons)] = loss_history\n",
        "\n",
        "    # Plotando as predições da rede em comparação com os dados reais\n",
        "    plot_predictions(x_samples, y_samples, updated_weights, updated_biases, activation_functions)\n",
        "\n",
        "# Plotando todas as curvas de perda em um único gráfico para comparação\n",
        "plt.figure(figsize=(10, 6))\n",
        "for neurons, loss_history in loss_histories_neurons.items():\n",
        "    plt.plot(loss_history, label=f\"Neurônios={neurons}\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Perda\")\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"Comparação das Curvas de Perda com Diferente Número de Neurônios por Camada\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vc7l6hrF57iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Tarefa 5: Introdução de Ruído nos Dados de Treinamento\n",
        "O\n",
        "bjetivo: Investigar como diferentes níveis de ruído nos dados de treinamento afetam a performance e a generalização da rede neural. Avaliaremos a robustez do modelo frente a dados ruidosos."
      ],
      "metadata": {
        "id": "3yZxDoH86J8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tarefa 5: Introdução de Ruído nos Dados de Treinamento\n",
        "\n",
        "# Definindo diferentes níveis de ruído (desvio padrão)\n",
        "noise_levels = [0.0, 0.1, 0.3, 0.5]\n",
        "\n",
        "# Armazenando os históricos de perda para cada configuração\n",
        "loss_histories_noise = {}\n",
        "\n",
        "for noise_std in noise_levels:\n",
        "    print(f\"\\nTreinando com nível de ruído: σ={noise_std}\")\n",
        "    # Gerando dados com o nível de ruído especificado\n",
        "    y_samples_noisy = np.sin(x_samples) + np.random.normal(0.0, noise_std, (1, N_SAMPLES))\n",
        "\n",
        "    # Visualizando os dados com ruído\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.scatter(x_samples.flatten(), y_samples_noisy.flatten(), label=f\"Dados com Ruído σ={noise_std}\", alpha=0.6)\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.legend()\n",
        "    plt.title(f\"Dados de Treinamento com Ruído σ={noise_std}\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Inicializando a rede neural com a função de ativação GELU\n",
        "    weights, biases, activation_functions, activations_derivatives = initialize_network(LAYERS, gelu, gelu_prime)\n",
        "\n",
        "    # Treinando a rede neural\n",
        "    loss_history, updated_weights, updated_biases = train_network(\n",
        "        n_epochs=N_EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        activation_functions=activation_functions,\n",
        "        activations_derivatives=activations_derivatives,\n",
        "        weights=weights,\n",
        "        biases=biases,\n",
        "        lambda_l2=0  # Sem regularização L2 nesta tarefa\n",
        "    )\n",
        "\n",
        "    # Armazenando o histórico de perda\n",
        "    loss_histories_noise[noise_std] = loss_history\n",
        "\n",
        "    # Plotando as predições da rede em comparação com os dados reais com ruído\n",
        "    plot_predictions(x_samples, y_samples_noisy, updated_weights, updated_biases, activation_functions)\n",
        "\n",
        "# Plotando todas as curvas de perda em um único gráfico para comparação\n",
        "plt.figure(figsize=(10, 6))\n",
        "for noise_std, loss_history in loss_histories_noise.items():\n",
        "    plt.plot(loss_history, label=f\"Ruído σ={noise_std}\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Perda\")\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"Comparação das Curvas de Perda com Diferente Níveis de Ruído\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QUA5ZUfq6KS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarefa 6: Aplicação de Regularização L2\n",
        "\n",
        "Objetivo: Avaliar como a aplicação da regularização L2 (lambda_l2) influencia o treinamento da rede neural, ajudando a prevenir o overfitting. Exploraremos diferentes valores de lambda para entender seu impacto no desempenho da rede."
      ],
      "metadata": {
        "id": "SHkZ0Gvp6Qdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tarefa 6: Aplicação de Regularização L2\n",
        "\n",
        "# Definindo diferentes valores de lambda para regularização L2\n",
        "lambda_l2_values = [0.0, 0.001, 0.01, 0.1]\n",
        "\n",
        "# Armazenando os históricos de perda para cada configuração\n",
        "loss_histories_l2 = {}\n",
        "\n",
        "for lambda_l2 in lambda_l2_values:\n",
        "    print(f\"\\nTreinando com Regularização L2: λ={lambda_l2}\")\n",
        "    # Inicializando a rede neural com a função de ativação GELU\n",
        "    weights, biases, activation_functions, activations_derivatives = initialize_network(LAYERS, gelu, gelu_prime)\n",
        "\n",
        "    # Treinando a rede neural com regularização L2\n",
        "    loss_history, updated_weights, updated_biases = train_network(\n",
        "        n_epochs=N_EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        activation_functions=activation_functions,\n",
        "        activations_derivatives=activations_derivatives,\n",
        "        weights=weights,\n",
        "        biases=biases,\n",
        "        lambda_l2=lambda_l2  # Aplicando regularização L2\n",
        "    )\n",
        "\n",
        "    # Armazenando o histórico de perda\n",
        "    loss_histories_l2[lambda_l2] = loss_history\n",
        "\n",
        "    # Plotando as predições da rede em comparação com os dados reais\n",
        "    plot_predictions(x_samples, y_samples, updated_weights, updated_biases, activation_functions)\n",
        "\n",
        "# Plotando todas as curvas de perda em um único gráfico para comparação\n",
        "plt.figure(figsize=(10, 6))\n",
        "for lambda_l2, loss_history in loss_histories_l2.items():\n",
        "    plt.plot(loss_history, label=f\"λ={lambda_l2}\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Perda\")\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"Comparação das Curvas de Perda com Diferentes Valores de Regularização L2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eyJlurDy6RFH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}